# 02 — I/O (CSV/Excel/Parquet/JSON/SQL) + dtypes

Real work starts with I/O. This chapter focuses on reading/writing files **correctly** and **predictably**, with attention to dtypes.

## Rule #1: Always inspect dtypes after reading

```python
import pandas as pd

df = pd.read_csv("data.csv")
print(df.dtypes)
```

## CSV

### Basic

```python
import pandas as pd

df = pd.read_csv("data.csv")
df.to_csv("output.csv", index=False)
```

### Common best-practice parameters

- `usecols`: load only what you need
- `dtype`: avoid expensive guessing + enforce schema
- `parse_dates`: parse timestamps (or use `pd.to_datetime` after)
- `na_values`: treat sentinel strings as missing

```python
import pandas as pd

df = pd.read_csv(
    "orders.csv",
    usecols=["order_id", "order_date", "customer", "quantity", "price"],
    dtype={"order_id": "int64", "customer": "string", "quantity": "Int64", "price": "float64"},
    parse_dates=["order_date"],
    na_values=["", "NA", "N/A", "null"],
)
```

### Chunking (files larger than memory)

```python
import pandas as pd

total = 0.0
for chunk in pd.read_csv("big.csv", usecols=["sales"], chunksize=100_000):
    total += chunk["sales"].sum()
print(total)
```

### `dtype_backend` (pandas 2.x)

Pandas 2.x supports `dtype_backend` for `read_csv`.

```python
import pandas as pd

# Experimental: returns nullable dtypes backed by PyArrow
# Requires pyarrow installed
# df = pd.read_csv("data.csv", dtype_backend="pyarrow")

# Nullable NumPy-backed dtypes
# df = pd.read_csv("data.csv", dtype_backend="numpy_nullable")
```

## Excel

```python
import pandas as pd

df = pd.read_excel("data.xlsx", sheet_name=0)
df.to_excel("output.xlsx", index=False)
```

If you read multiple sheets:

```python
import pandas as pd

sheets = pd.read_excel("data.xlsx", sheet_name=None)
print(sheets.keys())
```

## Parquet (recommended for analytics)

Parquet is columnar, compressed, and preserves types better than CSV.

```python
import pandas as pd

df = pd.read_parquet("data.parquet")
df.to_parquet("output.parquet", index=False)
```

## JSON

### Records

```python
import pandas as pd

df = pd.read_json("records.json", orient="records")
```

### JSON lines (common for logs)

```python
import pandas as pd

df = pd.read_json("events.jsonl", lines=True)
```

## SQL

### Read

```python
import pandas as pd
from sqlalchemy import create_engine

engine = create_engine("postgresql://user:pass@localhost/db")
df = pd.read_sql_query("SELECT id, created_at, amount FROM payments", engine)
```

### Write

```python
import pandas as pd
from sqlalchemy import create_engine

engine = create_engine("postgresql://user:pass@localhost/db")
df.to_sql("payments_snapshot", engine, if_exists="replace", index=False)
```

## Datetime parsing (robust)

Prefer explicit parsing if you care about correctness:

```python
import pandas as pd

df["created_at"] = pd.to_datetime(df["created_at"], errors="coerce", utc=True)
```

## Summary

- Always check `df.dtypes` after reading.
- Use `usecols`, `dtype`, and `parse_dates` to avoid surprises.
- Prefer Parquet for repeated reads/analytics.

## Next

Continue to [03 — Explore + inspect](03_explore_inspect.md).

[← Back to course home](../README.md)
